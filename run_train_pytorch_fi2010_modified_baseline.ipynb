{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428ce98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/lydiali/opt/anaconda3/lib/python3.9/site-packages (4.65.0)\n",
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lydiali/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/lydiali/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/lydiali/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "100%|███████████████████████| 1183/1183 [19:43<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset shape: (1089, 45)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install tqdm\n",
    "!pip install torchinfo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from datetime import datetime\n",
    "\n",
    "# === File paths ===\n",
    "snapshot_file = '../../datasets/Cong/BTCUSDT_orderbook_snapshot_2024_04_16-556.json'\n",
    "trade_file = '../../datasets/Cong/BTCUSDT_trade_2024_04_16-886.json'\n",
    "\n",
    "# === Load snapshot data ===\n",
    "with open(snapshot_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "parsed_data = [json.loads(line) for line in lines]\n",
    "\n",
    "snapshots = []\n",
    "for entry in parsed_data:\n",
    "    snapshots.append({\n",
    "        \"lastUpdateId\": entry[\"lastUpdateId\"],\n",
    "        \"asks\": entry.get(\"asks\", []),\n",
    "        \"bids\": entry.get(\"bids\", []),\n",
    "        \"timestamp\": entry.get(\"record_utc_time\")  # Optional: if available\n",
    "    })\n",
    "\n",
    "# === Load trade data ===\n",
    "with open(trade_file, \"r\") as f:\n",
    "    trade_lines = f.readlines()\n",
    "trade_data = [json.loads(line) for line in trade_lines]\n",
    "trade_records = [{\"U\": t[\"U\"], \"u\": t[\"u\"], \"a\": t[\"a\"], \"b\": t[\"b\"]} for t in trade_data]\n",
    "\n",
    "# === Match snapshots to trades and attach trade_price ===\n",
    "matched = []\n",
    "for snap in tqdm(snapshots):\n",
    "    snap_id = snap[\"lastUpdateId\"]\n",
    "    for t in trade_records:\n",
    "        if t[\"U\"] <= snap_id <= t[\"u\"]:\n",
    "            try:\n",
    "                best_ask = float(t[\"a\"][0][0])\n",
    "                best_bid = float(t[\"b\"][0][0])\n",
    "                trade_price = (best_ask + best_bid) / 2\n",
    "            except Exception:\n",
    "                continue\n",
    "            snap[\"trade_price\"] = trade_price\n",
    "            matched.append(snap)\n",
    "            break\n",
    "\n",
    "# === Feature engineering using snapshots ===\n",
    "records = []\n",
    "for snap in matched:\n",
    "    row = {\"timestamp\": snap.get(\"timestamp\")}\n",
    "    asks = sorted([[float(p), float(v)] for p, v in snap[\"asks\"]], key=lambda x: x[0])\n",
    "    bids = sorted([[float(p), float(v)] for p, v in snap[\"bids\"]], key=lambda x: -x[0])\n",
    "\n",
    "    for i in range(10):\n",
    "        row[f'ask_price_{i+1}'] = asks[i][0] if i < len(asks) else None\n",
    "        row[f'ask_volume_{i+1}'] = asks[i][1] if i < len(asks) else None\n",
    "        row[f'bid_price_{i+1}'] = bids[i][0] if i < len(bids) else None\n",
    "        row[f'bid_volume_{i+1}'] = bids[i][1] if i < len(bids) else None\n",
    "\n",
    "    row[\"mid_price\"] = (row[\"ask_price_1\"] + row[\"bid_price_1\"]) / 2\n",
    "    # row[\"spread\"] = row[\"ask_price_1\"] - row[\"bid_price_1\"]\n",
    "    # row[\"ask_depth\"] = sum([row[f'ask_volume_{j+1}'] for j in range(10) if row[f'ask_volume_{j+1}'] is not None])\n",
    "    # row[\"bid_depth\"] = sum([row[f'bid_volume_{j+1}'] for j in range(10) if row[f'bid_volume_{j+1}'] is not None])\n",
    "    # row[\"imbalance_5\"] = (\n",
    "    #     sum([row[f'bid_volume_{j+1}'] for j in range(5)]) -\n",
    "    #     sum([row[f'ask_volume_{j+1}'] for j in range(5)])\n",
    "    # ) / (\n",
    "    #     sum([row[f'bid_volume_{j+1}'] for j in range(5)]) +\n",
    "    #     sum([row[f'ask_volume_{j+1}'] for j in range(5)]) + 1e-8\n",
    "    # )\n",
    "    # row[\"imbalance_10\"] = (\n",
    "    #     sum([row[f'bid_volume_{j+1}'] for j in range(10)]) -\n",
    "    #     sum([row[f'ask_volume_{j+1}'] for j in range(10)])\n",
    "    # ) / (\n",
    "    #     sum([row[f'bid_volume_{j+1}'] for j in range(10)]) +\n",
    "    #     sum([row[f'ask_volume_{j+1}'] for j in range(10)]) + 1e-8\n",
    "    # )\n",
    "    row[\"trade_price\"] = snap[\"trade_price\"]\n",
    "    records.append(row)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df = df.dropna(subset=[\"ask_price_1\", \"bid_price_1\", \"trade_price\"]).reset_index(drop=True)\n",
    "\n",
    "# === Normalize features ===\n",
    "price_volume_cols = [col for col in df.columns if col.startswith(('ask_', 'bid_'))]\n",
    "scaler = StandardScaler()\n",
    "df_norm = pd.DataFrame(scaler.fit_transform(df[price_volume_cols]), columns=price_volume_cols)\n",
    "df_norm[\"trade_price\"] = df[\"trade_price\"]\n",
    "df_norm[\"mid_price_raw\"] = df[\"mid_price\"]\n",
    "df_norm[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"]) if \"timestamp\" in df else pd.NaT\n",
    "\n",
    "# === Step-based labeling using trade price ===\n",
    "step_horizons = [5, 10, 20, 30, 50]\n",
    "epsilon = 0.0001\n",
    "usable_df = df_norm.iloc[:-max(step_horizons)].copy()\n",
    "\n",
    "def label_movement(p_now, p_future):\n",
    "    delta = (p_future - p_now) / p_now\n",
    "    if delta > epsilon:\n",
    "        return 1\n",
    "    elif delta < -epsilon:\n",
    "        return 3\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "for h in step_horizons:\n",
    "    future_price = df_norm[\"trade_price\"].shift(-h)\n",
    "    usable_df[f\"label_{h}\"] = [\n",
    "        label_movement(p, f) if pd.notna(f) else np.nan\n",
    "        for p, f in zip(usable_df[\"trade_price\"], future_price)\n",
    "    ]\n",
    "\n",
    "label_cols = [f\"label_{h}\" for h in step_horizons]\n",
    "final_df = usable_df.dropna(subset=label_cols).reset_index(drop=True)\n",
    "\n",
    "# # === Coin Context Vector ===\n",
    "# df['log_return'] = np.log(df['mid_price']).diff()\n",
    "# coin_context = {\n",
    "#     \"tick_size\": 0.01,\n",
    "#     \"spread_median\": df['spread'].median(),\n",
    "#     \"spread_std\": df['spread'].std(),\n",
    "#     \"ask_depth_mean\": df['ask_depth'].mean(),\n",
    "#     \"ask_depth_std\": df['ask_depth'].std(),\n",
    "#     \"bid_depth_mean\": df['bid_depth'].mean(),\n",
    "#     \"bid_depth_std\": df['bid_depth'].std(),\n",
    "#     \"imbalance_5_mean\": df['imbalance_5'].mean(),\n",
    "#     \"imbalance_5_std\": df['imbalance_5'].std(),\n",
    "# }\n",
    "\n",
    "# for h in step_horizons:\n",
    "#     coin_context[f\"realized_vol_{h}\"] = df['log_return'].rolling(window=h).std().mean()\n",
    "#     coin_context[f\"price_range_{h}\"] = (df['mid_price'].rolling(window=h).max() - df['mid_price'].rolling(window=h).min()).mean()\n",
    "\n",
    "# coin_context_df = pd.DataFrame([coin_context])\n",
    "# context_scaler = StandardScaler()\n",
    "# context_numeric = coin_context_df.select_dtypes(include=[np.number])\n",
    "# context_normalized = pd.DataFrame(\n",
    "#     context_scaler.fit_transform(context_numeric),\n",
    "#     columns=context_numeric.columns\n",
    "# )\n",
    "\n",
    "# === Merge context with final_df ===\n",
    "# context_broadcast = pd.concat([context_normalized] * len(final_df), ignore_index=True)\n",
    "# context_broadcast.index = final_df.index\n",
    "# final_df = pd.concat([final_df.drop(columns=label_cols), context_broadcast, final_df[label_cols]], axis=1)\n",
    "\n",
    "final_df = final_df.drop(columns=[\"timestamp\"])\n",
    "final_df = final_df.drop(columns=[\"trade_price\"])\n",
    "final_df = final_df.drop(columns=[\"mid_price_raw\"])\n",
    "\n",
    "\n",
    "# === Done ===\n",
    "# You can now use final_df for training\n",
    "# final_df.to_csv(\"snapshot_trade_labeled_with_context.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # === Load JSON snapshot data ===\n",
    "# json_path = '../../datasets/Cong/BTCUSDT_orderbook_snapshot_2024_04_16-556.json'\n",
    "# with open(json_path, \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# parsed_data = [json.loads(line) for line in lines]\n",
    "# snapshots = []\n",
    "# for entry in parsed_data:\n",
    "#     snapshots.append({\n",
    "#         \"timestamp\": entry.get(\"record_utc_time\"),\n",
    "#         \"asks\": entry.get(\"asks\", []),\n",
    "#         \"bids\": entry.get(\"bids\", [])\n",
    "#     })\n",
    "\n",
    "# # === Feature Engineering ===\n",
    "# records = []\n",
    "# for snap in snapshots:\n",
    "#     row = {\"timestamp\": snap[\"timestamp\"]}\n",
    "#     asks = sorted([[float(p), float(v)] for p, v in snap[\"asks\"]], key=lambda x: x[0])\n",
    "#     bids = sorted([[float(p), float(v)] for p, v in snap[\"bids\"]], key=lambda x: -x[0])\n",
    "\n",
    "#     for i in range(10):\n",
    "#         row[f'ask_price_{i+1}'] = asks[i][0] if i < len(asks) else None\n",
    "#         row[f'ask_volume_{i+1}'] = asks[i][1] if i < len(asks) else None\n",
    "#         row[f'bid_price_{i+1}'] = bids[i][0] if i < len(bids) else None\n",
    "#         row[f'bid_volume_{i+1}'] = bids[i][1] if i < len(bids) else None\n",
    "\n",
    "#     row[\"mid_price\"] = (row[\"ask_price_1\"] + row[\"bid_price_1\"]) / 2\n",
    "#     row[\"spread\"] = row[\"ask_price_1\"] - row[\"bid_price_1\"]\n",
    "#     row[\"ask_depth\"] = sum([row[f'ask_volume_{j+1}'] for j in range(10) if row[f'ask_volume_{j+1}'] is not None])\n",
    "#     row[\"bid_depth\"] = sum([row[f'bid_volume_{j+1}'] for j in range(10) if row[f'bid_volume_{j+1}'] is not None])\n",
    "#     row[\"imbalance_5\"] = (\n",
    "#         sum([row[f'bid_volume_{j+1}'] for j in range(5)]) -\n",
    "#         sum([row[f'ask_volume_{j+1}'] for j in range(5)])\n",
    "#     ) / (\n",
    "#         sum([row[f'bid_volume_{j+1}'] for j in range(5)]) +\n",
    "#         sum([row[f'ask_volume_{j+1}'] for j in range(5)]) + 1e-8\n",
    "#     )\n",
    "#     row[\"imbalance_10\"] = (\n",
    "#         sum([row[f'bid_volume_{j+1}'] for j in range(10)]) -\n",
    "#         sum([row[f'ask_volume_{j+1}'] for j in range(10)])\n",
    "#     ) / (\n",
    "#         sum([row[f'bid_volume_{j+1}'] for j in range(10)]) +\n",
    "#         sum([row[f'ask_volume_{j+1}'] for j in range(10)]) + 1e-8\n",
    "#     )\n",
    "#     records.append(row)\n",
    "\n",
    "# df = pd.DataFrame(records)\n",
    "# df = df.dropna(subset=[\"ask_price_1\", \"bid_price_1\"]).reset_index(drop=True)\n",
    "# df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "# # === Normalize numerical features ===\n",
    "# price_volume_cols = [col for col in df.columns if col.startswith(('ask_', 'bid_')) or col in [\n",
    "#     \"mid_price\", \"spread\", \"ask_depth\", \"bid_depth\", \"imbalance_5\", \"imbalance_10\"\n",
    "# ]]\n",
    "# scaler = StandardScaler()\n",
    "# df_norm = pd.DataFrame(scaler.fit_transform(df[price_volume_cols]), columns=price_volume_cols)\n",
    "# df_norm[\"timestamp\"] = df[\"timestamp\"]\n",
    "# df_norm[\"mid_price_raw\"] = df[\"mid_price\"]\n",
    "\n",
    "# # === Step-based Labeling ===\n",
    "# step_horizons = [5, 10, 20, 30, 50]\n",
    "# epsilon = 0.0001\n",
    "\n",
    "# def label_movement(p_now, p_future):\n",
    "#     delta = (p_future - p_now) / p_now\n",
    "#     if delta > epsilon:\n",
    "#         return 1\n",
    "#     elif delta < -epsilon:\n",
    "#         return 3\n",
    "#     else:\n",
    "#         return 2\n",
    "\n",
    "# usable_df = df_norm.iloc[:-max(step_horizons)].copy()\n",
    "# for h in step_horizons:\n",
    "#     future_mid = df_norm[\"mid_price_raw\"].shift(-h)\n",
    "#     usable_df[f\"label_{h}\"] = [\n",
    "#         label_movement(p, f) if pd.notna(f) else np.nan\n",
    "#         for p, f in zip(usable_df[\"mid_price_raw\"], future_mid)\n",
    "#     ]\n",
    "# label_cols = [f\"label_{h}\" for h in step_horizons]\n",
    "# final_df = usable_df.dropna(subset=label_cols).reset_index(drop=True)\n",
    "\n",
    "# # === Coin Context Vector ===\n",
    "# df['log_return'] = np.log(df['mid_price']).diff()\n",
    "# coin_context = {\n",
    "#     \"tick_size\": 0.01,\n",
    "#     # \"exchange\": \"Binance\",\n",
    "#     \"spread_median\": df['spread'].median(),\n",
    "#     \"spread_std\": df['spread'].std(),\n",
    "#     \"ask_depth_mean\": df['ask_depth'].mean(),\n",
    "#     \"ask_depth_std\": df['ask_depth'].std(),\n",
    "#     \"bid_depth_mean\": df['bid_depth'].mean(),›\n",
    "#     \"bid_depth_std\": df['bid_depth'].std(),\n",
    "#     \"imbalance_5_mean\": df['imbalance_5'].mean(),\n",
    "#     \"imbalance_5_std\": df['imbalance_5'].std(),\n",
    "#     # \"update_frequency_sec\": df['timestamp'].diff().dt.total_seconds().dropna().mean(),\n",
    "# }\n",
    "\n",
    "# # Add volatility and price range for each horizon\n",
    "# for h in step_horizons:\n",
    "#     coin_context[f\"realized_vol_{h}\"] = df['log_return'].rolling(window=h).std().mean()\n",
    "#     coin_context[f\"price_range_{h}\"] = (df['mid_price'].rolling(window=h).max() - df['mid_price'].rolling(window=h).min()).mean()\n",
    "\n",
    "# coin_context_df = pd.DataFrame([coin_context])\n",
    "\n",
    "# # === Normalize coin context vector ===\n",
    "# # context_numeric = coin_context_df.drop(columns=[\"exchange\"])\n",
    "# context_scaler = StandardScaler()\n",
    "# context_normalized = pd.DataFrame(context_scaler.fit_transform(context_numeric), columns=context_numeric.columns)\n",
    "# # context_normalized[\"exchange\"] = coin_context_df[\"exchange\"].values  # Add back exchange (if needed for reference)\n",
    "\n",
    "# # === Merge with final_df ===\n",
    "# context_broadcast = pd.concat([context_normalized] * len(final_df), ignore_index=True)\n",
    "# context_broadcast.index = final_df.index\n",
    "# merged_df = pd.concat([final_df.drop(columns=label_cols), context_broadcast, final_df[label_cols]], axis=1)\n",
    "\n",
    "# # Your final dataframe\n",
    "# final_df = merged_df\n",
    "\n",
    "# final_df = final_df.drop(columns=[\"timestamp\"])\n",
    "\n",
    "\n",
    "# # Load JSON snapshot data\n",
    "# json_path = '../../datasets/Cong/BTCUSDT_orderbook_snapshot_2024_04_16-556.json'\n",
    "# with open(json_path, \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# parsed_data = [json.loads(line) for line in lines]\n",
    "# snapshots = []\n",
    "# for entry in parsed_data:\n",
    "#     snapshots.append({\n",
    "#         \"timestamp\": entry.get(\"record_utc_time\"),\n",
    "#         \"asks\": entry.get(\"asks\", []),\n",
    "#         \"bids\": entry.get(\"bids\", [])\n",
    "#     })\n",
    "\n",
    "# records = []\n",
    "# for snap in snapshots:\n",
    "#     row = {\"timestamp\": snap[\"timestamp\"]}\n",
    "#     for i in range(10):\n",
    "#         if i < len(snap[\"asks\"]):\n",
    "#             row[f'ask_price_{i+1}'] = float(snap[\"asks\"][i][0])\n",
    "#             row[f'ask_volume_{i+1}'] = float(snap[\"asks\"][i][1])\n",
    "#         else:\n",
    "#             row[f'ask_price_{i+1}'] = None\n",
    "#             row[f'ask_volume_{i+1}'] = None\n",
    "#         if i < len(snap[\"bids\"]):\n",
    "#             row[f'bid_price_{i+1}'] = float(snap[\"bids\"][i][0])\n",
    "#             row[f'bid_volume_{i+1}'] = float(snap[\"bids\"][i][1])\n",
    "#         else:\n",
    "#             row[f'bid_price_{i+1}'] = None\n",
    "#             row[f'bid_volume_{i+1}'] = None\n",
    "#     records.append(row)\n",
    "\n",
    "# df = pd.DataFrame(records)\n",
    "# df = df.dropna(subset=[\"ask_price_1\", \"bid_price_1\"]).reset_index(drop=True)\n",
    "\n",
    "# # Normalize features\n",
    "# price_volume_cols = [col for col in df.columns if col.startswith('ask_price_') or \n",
    "#                      col.startswith('bid_price_') or col.startswith('ask_volume_') or \n",
    "#                      col.startswith('bid_volume_')]\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# df_norm = pd.DataFrame(scaler.fit_transform(df[price_volume_cols]), columns=price_volume_cols)\n",
    "# df_norm[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "# df_norm[\"mid_price\"] = (df[\"ask_price_1\"] + df[\"bid_price_1\"]) / 2\n",
    "\n",
    "# # Multi-horizon label generation\n",
    "# horizons = [5, 10, 20, 30, 50]\n",
    "# # horizons = [10, 20, 30, 50, 100]\n",
    "# epsilon = 0.0001\n",
    "\n",
    "# # Use 3-class labeling\n",
    "# def label_movement(p_now, p_future):\n",
    "#     delta = (p_future - p_now) / p_now\n",
    "#     if delta > epsilon:\n",
    "#         return 1  # Up\n",
    "#     elif delta < -epsilon:\n",
    "#         return 3  # Down\n",
    "#     else:\n",
    "#         return 2  # Stationary\n",
    "\n",
    "# usable_df = df_norm.iloc[:-max(horizons)].copy()\n",
    "# for h in horizons:\n",
    "#     future_mid = df_norm[\"mid_price\"].shift(-h)\n",
    "#     usable_df[f\"label_{h}\"] = [\n",
    "#         label_movement(p, f) if pd.notna(f) else np.nan\n",
    "#         for p, f in zip(usable_df[\"mid_price\"], future_mid)\n",
    "#     ]\n",
    "\n",
    "# label_cols = [f\"label_{h}\" for h in horizons]\n",
    "# final_df = usable_df.dropna(subset=label_cols).reset_index(drop=True)\n",
    "print(\"Processed dataset shape:\", final_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a32448fd-cad3-4f6c-a3d1-2c0e8ddae6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205f4759-c219-4e0a-bb4b-03433e5d638e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1079</th>\n",
       "      <th>1080</th>\n",
       "      <th>1081</th>\n",
       "      <th>1082</th>\n",
       "      <th>1083</th>\n",
       "      <th>1084</th>\n",
       "      <th>1085</th>\n",
       "      <th>1086</th>\n",
       "      <th>1087</th>\n",
       "      <th>1088</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ask_price_1</th>\n",
       "      <td>0.787777</td>\n",
       "      <td>0.758016</td>\n",
       "      <td>0.939000</td>\n",
       "      <td>0.793726</td>\n",
       "      <td>1.009654</td>\n",
       "      <td>0.764239</td>\n",
       "      <td>0.765947</td>\n",
       "      <td>0.691661</td>\n",
       "      <td>0.733535</td>\n",
       "      <td>0.778020</td>\n",
       "      <td>...</td>\n",
       "      <td>1.358018</td>\n",
       "      <td>1.361493</td>\n",
       "      <td>1.464755</td>\n",
       "      <td>1.399127</td>\n",
       "      <td>1.574732</td>\n",
       "      <td>1.473707</td>\n",
       "      <td>1.508063</td>\n",
       "      <td>1.498541</td>\n",
       "      <td>1.518134</td>\n",
       "      <td>1.556828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_volume_1</th>\n",
       "      <td>-0.889437</td>\n",
       "      <td>0.309400</td>\n",
       "      <td>0.260860</td>\n",
       "      <td>0.596678</td>\n",
       "      <td>-0.794347</td>\n",
       "      <td>-0.399067</td>\n",
       "      <td>0.278482</td>\n",
       "      <td>0.051446</td>\n",
       "      <td>-1.074707</td>\n",
       "      <td>-0.853522</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871834</td>\n",
       "      <td>1.339390</td>\n",
       "      <td>1.550618</td>\n",
       "      <td>2.036066</td>\n",
       "      <td>0.100610</td>\n",
       "      <td>-1.124067</td>\n",
       "      <td>0.054699</td>\n",
       "      <td>1.058963</td>\n",
       "      <td>1.625680</td>\n",
       "      <td>-0.937288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price_1</th>\n",
       "      <td>0.787798</td>\n",
       "      <td>0.758037</td>\n",
       "      <td>0.939019</td>\n",
       "      <td>0.793746</td>\n",
       "      <td>1.009672</td>\n",
       "      <td>0.764260</td>\n",
       "      <td>0.765968</td>\n",
       "      <td>0.691683</td>\n",
       "      <td>0.733557</td>\n",
       "      <td>0.778041</td>\n",
       "      <td>...</td>\n",
       "      <td>1.358031</td>\n",
       "      <td>1.361505</td>\n",
       "      <td>1.464766</td>\n",
       "      <td>1.399139</td>\n",
       "      <td>1.574741</td>\n",
       "      <td>1.473718</td>\n",
       "      <td>1.508073</td>\n",
       "      <td>1.498552</td>\n",
       "      <td>1.518144</td>\n",
       "      <td>1.556838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_volume_1</th>\n",
       "      <td>0.369989</td>\n",
       "      <td>-0.802514</td>\n",
       "      <td>-0.789181</td>\n",
       "      <td>-0.867463</td>\n",
       "      <td>0.505927</td>\n",
       "      <td>-0.957800</td>\n",
       "      <td>-0.503068</td>\n",
       "      <td>-1.089764</td>\n",
       "      <td>0.068876</td>\n",
       "      <td>1.929023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765229</td>\n",
       "      <td>-0.769640</td>\n",
       "      <td>-0.715331</td>\n",
       "      <td>-0.693146</td>\n",
       "      <td>-0.452891</td>\n",
       "      <td>-0.307373</td>\n",
       "      <td>0.014105</td>\n",
       "      <td>-0.765127</td>\n",
       "      <td>-0.956201</td>\n",
       "      <td>1.235708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_2</th>\n",
       "      <td>0.787888</td>\n",
       "      <td>0.757887</td>\n",
       "      <td>0.938915</td>\n",
       "      <td>0.793877</td>\n",
       "      <td>1.009834</td>\n",
       "      <td>0.763758</td>\n",
       "      <td>0.765466</td>\n",
       "      <td>0.691189</td>\n",
       "      <td>0.733266</td>\n",
       "      <td>0.777541</td>\n",
       "      <td>...</td>\n",
       "      <td>1.359226</td>\n",
       "      <td>1.361327</td>\n",
       "      <td>1.464505</td>\n",
       "      <td>1.398750</td>\n",
       "      <td>1.574634</td>\n",
       "      <td>1.474931</td>\n",
       "      <td>1.507936</td>\n",
       "      <td>1.498159</td>\n",
       "      <td>1.518127</td>\n",
       "      <td>1.557788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_volume_2</th>\n",
       "      <td>-0.125636</td>\n",
       "      <td>-0.244460</td>\n",
       "      <td>-0.244460</td>\n",
       "      <td>-0.434931</td>\n",
       "      <td>-0.246263</td>\n",
       "      <td>-0.028807</td>\n",
       "      <td>-0.447361</td>\n",
       "      <td>-0.447361</td>\n",
       "      <td>-0.447361</td>\n",
       "      <td>-0.400926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.447361</td>\n",
       "      <td>0.058041</td>\n",
       "      <td>0.196378</td>\n",
       "      <td>-0.245426</td>\n",
       "      <td>-0.434931</td>\n",
       "      <td>-0.447361</td>\n",
       "      <td>0.057461</td>\n",
       "      <td>-0.087993</td>\n",
       "      <td>-0.245426</td>\n",
       "      <td>-0.245426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price_2</th>\n",
       "      <td>0.788259</td>\n",
       "      <td>0.758442</td>\n",
       "      <td>0.938029</td>\n",
       "      <td>0.794147</td>\n",
       "      <td>1.009990</td>\n",
       "      <td>0.764017</td>\n",
       "      <td>0.765823</td>\n",
       "      <td>0.691919</td>\n",
       "      <td>0.734023</td>\n",
       "      <td>0.778052</td>\n",
       "      <td>...</td>\n",
       "      <td>1.358191</td>\n",
       "      <td>1.361724</td>\n",
       "      <td>1.465150</td>\n",
       "      <td>1.399530</td>\n",
       "      <td>1.574308</td>\n",
       "      <td>1.473806</td>\n",
       "      <td>1.508373</td>\n",
       "      <td>1.498932</td>\n",
       "      <td>1.518522</td>\n",
       "      <td>1.557034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_volume_2</th>\n",
       "      <td>-0.133476</td>\n",
       "      <td>-0.134059</td>\n",
       "      <td>0.074742</td>\n",
       "      <td>0.062083</td>\n",
       "      <td>-0.029738</td>\n",
       "      <td>-0.092298</td>\n",
       "      <td>-0.133178</td>\n",
       "      <td>-0.119246</td>\n",
       "      <td>-0.132025</td>\n",
       "      <td>-0.029546</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131514</td>\n",
       "      <td>-0.134059</td>\n",
       "      <td>-0.128592</td>\n",
       "      <td>-0.118570</td>\n",
       "      <td>-0.092497</td>\n",
       "      <td>-0.092497</td>\n",
       "      <td>0.042280</td>\n",
       "      <td>-0.092497</td>\n",
       "      <td>-0.092371</td>\n",
       "      <td>-0.119883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_3</th>\n",
       "      <td>0.787568</td>\n",
       "      <td>0.758251</td>\n",
       "      <td>0.938615</td>\n",
       "      <td>0.793774</td>\n",
       "      <td>1.009680</td>\n",
       "      <td>0.763297</td>\n",
       "      <td>0.765163</td>\n",
       "      <td>0.690837</td>\n",
       "      <td>0.734156</td>\n",
       "      <td>0.777396</td>\n",
       "      <td>...</td>\n",
       "      <td>1.359255</td>\n",
       "      <td>1.363712</td>\n",
       "      <td>1.464233</td>\n",
       "      <td>1.399314</td>\n",
       "      <td>1.574219</td>\n",
       "      <td>1.476094</td>\n",
       "      <td>1.507572</td>\n",
       "      <td>1.498814</td>\n",
       "      <td>1.519629</td>\n",
       "      <td>1.557587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_volume_3</th>\n",
       "      <td>-0.262882</td>\n",
       "      <td>0.275781</td>\n",
       "      <td>-0.506480</td>\n",
       "      <td>-0.451355</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>-0.247772</td>\n",
       "      <td>1.895348</td>\n",
       "      <td>0.078651</td>\n",
       "      <td>-0.262882</td>\n",
       "      <td>0.911670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120005</td>\n",
       "      <td>-0.264062</td>\n",
       "      <td>-0.495738</td>\n",
       "      <td>-0.234827</td>\n",
       "      <td>-0.264062</td>\n",
       "      <td>-0.510808</td>\n",
       "      <td>-0.420074</td>\n",
       "      <td>-0.264062</td>\n",
       "      <td>-0.264062</td>\n",
       "      <td>-0.117337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price_3</th>\n",
       "      <td>0.788758</td>\n",
       "      <td>0.758572</td>\n",
       "      <td>0.938428</td>\n",
       "      <td>0.794547</td>\n",
       "      <td>1.010064</td>\n",
       "      <td>0.764441</td>\n",
       "      <td>0.765736</td>\n",
       "      <td>0.692373</td>\n",
       "      <td>0.734314</td>\n",
       "      <td>0.778277</td>\n",
       "      <td>...</td>\n",
       "      <td>1.357273</td>\n",
       "      <td>1.362101</td>\n",
       "      <td>1.465473</td>\n",
       "      <td>1.399862</td>\n",
       "      <td>1.573928</td>\n",
       "      <td>1.474206</td>\n",
       "      <td>1.508768</td>\n",
       "      <td>1.499171</td>\n",
       "      <td>1.518758</td>\n",
       "      <td>1.557402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_volume_3</th>\n",
       "      <td>-0.404985</td>\n",
       "      <td>0.829854</td>\n",
       "      <td>-0.116821</td>\n",
       "      <td>-0.125072</td>\n",
       "      <td>0.086748</td>\n",
       "      <td>0.363772</td>\n",
       "      <td>-0.241577</td>\n",
       "      <td>-0.397280</td>\n",
       "      <td>-0.216279</td>\n",
       "      <td>-0.216279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.418565</td>\n",
       "      <td>-0.360682</td>\n",
       "      <td>0.437514</td>\n",
       "      <td>-0.376509</td>\n",
       "      <td>-0.418565</td>\n",
       "      <td>-0.043240</td>\n",
       "      <td>9.708950</td>\n",
       "      <td>-0.411983</td>\n",
       "      <td>-0.389768</td>\n",
       "      <td>2.325632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_4</th>\n",
       "      <td>0.787467</td>\n",
       "      <td>0.757949</td>\n",
       "      <td>0.938122</td>\n",
       "      <td>0.793320</td>\n",
       "      <td>1.009197</td>\n",
       "      <td>0.763075</td>\n",
       "      <td>0.764646</td>\n",
       "      <td>0.690310</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.777058</td>\n",
       "      <td>...</td>\n",
       "      <td>1.358820</td>\n",
       "      <td>1.363376</td>\n",
       "      <td>1.463812</td>\n",
       "      <td>1.398884</td>\n",
       "      <td>1.574952</td>\n",
       "      <td>1.477187</td>\n",
       "      <td>1.507176</td>\n",
       "      <td>1.498496</td>\n",
       "      <td>1.519490</td>\n",
       "      <td>1.557179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_volume_4</th>\n",
       "      <td>0.058202</td>\n",
       "      <td>-0.246954</td>\n",
       "      <td>-0.396185</td>\n",
       "      <td>-0.235855</td>\n",
       "      <td>0.057952</td>\n",
       "      <td>-0.235855</td>\n",
       "      <td>21.450375</td>\n",
       "      <td>0.641376</td>\n",
       "      <td>-0.461455</td>\n",
       "      <td>0.182155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494580</td>\n",
       "      <td>-0.318562</td>\n",
       "      <td>-0.236929</td>\n",
       "      <td>0.662429</td>\n",
       "      <td>-0.318562</td>\n",
       "      <td>1.005394</td>\n",
       "      <td>-0.361169</td>\n",
       "      <td>-0.042872</td>\n",
       "      <td>0.483302</td>\n",
       "      <td>-0.365680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price_4</th>\n",
       "      <td>0.789295</td>\n",
       "      <td>0.758817</td>\n",
       "      <td>0.937796</td>\n",
       "      <td>0.794554</td>\n",
       "      <td>1.010428</td>\n",
       "      <td>0.764293</td>\n",
       "      <td>0.766275</td>\n",
       "      <td>0.692740</td>\n",
       "      <td>0.734855</td>\n",
       "      <td>0.778815</td>\n",
       "      <td>...</td>\n",
       "      <td>1.357768</td>\n",
       "      <td>1.362341</td>\n",
       "      <td>1.465901</td>\n",
       "      <td>1.400295</td>\n",
       "      <td>1.573779</td>\n",
       "      <td>1.473633</td>\n",
       "      <td>1.507741</td>\n",
       "      <td>1.499656</td>\n",
       "      <td>1.519045</td>\n",
       "      <td>1.557549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_volume_4</th>\n",
       "      <td>-0.305116</td>\n",
       "      <td>-0.199794</td>\n",
       "      <td>-0.199794</td>\n",
       "      <td>-0.221749</td>\n",
       "      <td>0.495750</td>\n",
       "      <td>-0.370369</td>\n",
       "      <td>-0.199794</td>\n",
       "      <td>-0.356205</td>\n",
       "      <td>-0.013859</td>\n",
       "      <td>0.520098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506714</td>\n",
       "      <td>-0.286667</td>\n",
       "      <td>-0.366334</td>\n",
       "      <td>-0.281325</td>\n",
       "      <td>-0.222695</td>\n",
       "      <td>-0.097393</td>\n",
       "      <td>0.323005</td>\n",
       "      <td>-0.373096</td>\n",
       "      <td>-0.375127</td>\n",
       "      <td>-0.200629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_5</th>\n",
       "      <td>0.787550</td>\n",
       "      <td>0.757479</td>\n",
       "      <td>0.937631</td>\n",
       "      <td>0.792814</td>\n",
       "      <td>1.008930</td>\n",
       "      <td>0.763882</td>\n",
       "      <td>0.764256</td>\n",
       "      <td>0.690247</td>\n",
       "      <td>0.733929</td>\n",
       "      <td>0.776708</td>\n",
       "      <td>...</td>\n",
       "      <td>1.359432</td>\n",
       "      <td>1.365580</td>\n",
       "      <td>1.463355</td>\n",
       "      <td>1.399305</td>\n",
       "      <td>1.574625</td>\n",
       "      <td>1.476771</td>\n",
       "      <td>1.507077</td>\n",
       "      <td>1.498455</td>\n",
       "      <td>1.519039</td>\n",
       "      <td>1.557615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_volume_5</th>\n",
       "      <td>-0.224638</td>\n",
       "      <td>0.007922</td>\n",
       "      <td>-0.415813</td>\n",
       "      <td>-0.427709</td>\n",
       "      <td>-0.427709</td>\n",
       "      <td>0.242191</td>\n",
       "      <td>-0.344202</td>\n",
       "      <td>-0.216562</td>\n",
       "      <td>-0.397014</td>\n",
       "      <td>-0.427709</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.427709</td>\n",
       "      <td>0.233478</td>\n",
       "      <td>2.406304</td>\n",
       "      <td>-0.217567</td>\n",
       "      <td>-0.427709</td>\n",
       "      <td>0.550852</td>\n",
       "      <td>1.230201</td>\n",
       "      <td>0.623334</td>\n",
       "      <td>-0.129737</td>\n",
       "      <td>-0.217567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price_5</th>\n",
       "      <td>0.789589</td>\n",
       "      <td>0.758447</td>\n",
       "      <td>0.937783</td>\n",
       "      <td>0.795064</td>\n",
       "      <td>1.010585</td>\n",
       "      <td>0.764727</td>\n",
       "      <td>0.766473</td>\n",
       "      <td>0.692475</td>\n",
       "      <td>0.734291</td>\n",
       "      <td>0.778188</td>\n",
       "      <td>...</td>\n",
       "      <td>1.357384</td>\n",
       "      <td>1.361878</td>\n",
       "      <td>1.465978</td>\n",
       "      <td>1.400751</td>\n",
       "      <td>1.574003</td>\n",
       "      <td>1.473592</td>\n",
       "      <td>1.508109</td>\n",
       "      <td>1.499965</td>\n",
       "      <td>1.517881</td>\n",
       "      <td>1.556971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_volume_5</th>\n",
       "      <td>0.966756</td>\n",
       "      <td>-0.446210</td>\n",
       "      <td>0.110235</td>\n",
       "      <td>-0.223583</td>\n",
       "      <td>-0.163872</td>\n",
       "      <td>-0.446175</td>\n",
       "      <td>0.868956</td>\n",
       "      <td>-0.446210</td>\n",
       "      <td>0.012506</td>\n",
       "      <td>-0.223583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.933614</td>\n",
       "      <td>-0.048088</td>\n",
       "      <td>0.075503</td>\n",
       "      <td>0.966756</td>\n",
       "      <td>-0.330250</td>\n",
       "      <td>0.495319</td>\n",
       "      <td>-0.446210</td>\n",
       "      <td>-0.446210</td>\n",
       "      <td>-0.446210</td>\n",
       "      <td>-0.446210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_6</th>\n",
       "      <td>0.787083</td>\n",
       "      <td>0.757618</td>\n",
       "      <td>0.937101</td>\n",
       "      <td>0.792682</td>\n",
       "      <td>1.008603</td>\n",
       "      <td>0.763511</td>\n",
       "      <td>0.763747</td>\n",
       "      <td>0.689671</td>\n",
       "      <td>0.733633</td>\n",
       "      <td>0.776299</td>\n",
       "      <td>...</td>\n",
       "      <td>1.360536</td>\n",
       "      <td>1.365074</td>\n",
       "      <td>1.463056</td>\n",
       "      <td>1.399666</td>\n",
       "      <td>1.574611</td>\n",
       "      <td>1.476296</td>\n",
       "      <td>1.506822</td>\n",
       "      <td>1.499121</td>\n",
       "      <td>1.518686</td>\n",
       "      <td>1.557816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_volume_6</th>\n",
       "      <td>-0.245052</td>\n",
       "      <td>-0.245052</td>\n",
       "      <td>-0.505852</td>\n",
       "      <td>0.185717</td>\n",
       "      <td>0.681970</td>\n",
       "      <td>0.780493</td>\n",
       "      <td>-0.000361</td>\n",
       "      <td>0.059754</td>\n",
       "      <td>1.320388</td>\n",
       "      <td>-0.485056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.531791</td>\n",
       "      <td>7.431290</td>\n",
       "      <td>-0.527422</td>\n",
       "      <td>-0.531791</td>\n",
       "      <td>-0.527923</td>\n",
       "      <td>-0.531791</td>\n",
       "      <td>-0.246417</td>\n",
       "      <td>-0.531791</td>\n",
       "      <td>-0.531791</td>\n",
       "      <td>-0.531791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price_6</th>\n",
       "      <td>0.788530</td>\n",
       "      <td>0.757920</td>\n",
       "      <td>0.938107</td>\n",
       "      <td>0.795025</td>\n",
       "      <td>1.011041</td>\n",
       "      <td>0.765160</td>\n",
       "      <td>0.766181</td>\n",
       "      <td>0.692442</td>\n",
       "      <td>0.734766</td>\n",
       "      <td>0.778287</td>\n",
       "      <td>...</td>\n",
       "      <td>1.357857</td>\n",
       "      <td>1.362350</td>\n",
       "      <td>1.466268</td>\n",
       "      <td>1.401104</td>\n",
       "      <td>1.574364</td>\n",
       "      <td>1.473724</td>\n",
       "      <td>1.508258</td>\n",
       "      <td>1.500135</td>\n",
       "      <td>1.516735</td>\n",
       "      <td>1.557431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_volume_6</th>\n",
       "      <td>-0.161394</td>\n",
       "      <td>-0.203061</td>\n",
       "      <td>-0.334344</td>\n",
       "      <td>-0.091619</td>\n",
       "      <td>-0.203061</td>\n",
       "      <td>0.167475</td>\n",
       "      <td>-0.215503</td>\n",
       "      <td>-0.463064</td>\n",
       "      <td>-0.203061</td>\n",
       "      <td>-0.463519</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267132</td>\n",
       "      <td>0.007877</td>\n",
       "      <td>-0.204301</td>\n",
       "      <td>-0.463519</td>\n",
       "      <td>1.634821</td>\n",
       "      <td>-0.204301</td>\n",
       "      <td>-0.204301</td>\n",
       "      <td>0.573643</td>\n",
       "      <td>-0.463519</td>\n",
       "      <td>0.184258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_7</th>\n",
       "      <td>0.787987</td>\n",
       "      <td>0.757890</td>\n",
       "      <td>0.936605</td>\n",
       "      <td>0.792840</td>\n",
       "      <td>1.008546</td>\n",
       "      <td>0.762998</td>\n",
       "      <td>0.763372</td>\n",
       "      <td>0.690212</td>\n",
       "      <td>0.733118</td>\n",
       "      <td>0.775945</td>\n",
       "      <td>...</td>\n",
       "      <td>1.361692</td>\n",
       "      <td>1.364914</td>\n",
       "      <td>1.463239</td>\n",
       "      <td>1.399215</td>\n",
       "      <td>1.574275</td>\n",
       "      <td>1.475930</td>\n",
       "      <td>1.506813</td>\n",
       "      <td>1.498679</td>\n",
       "      <td>1.518246</td>\n",
       "      <td>1.558028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_volume_7</th>\n",
       "      <td>-0.079990</td>\n",
       "      <td>0.082992</td>\n",
       "      <td>-0.583127</td>\n",
       "      <td>-0.279858</td>\n",
       "      <td>-0.279858</td>\n",
       "      <td>0.461658</td>\n",
       "      <td>-0.279858</td>\n",
       "      <td>4.405615</td>\n",
       "      <td>-0.279858</td>\n",
       "      <td>-0.279858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.621151</td>\n",
       "      <td>-0.281483</td>\n",
       "      <td>-0.025825</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>-0.617197</td>\n",
       "      <td>-0.034058</td>\n",
       "      <td>-0.621151</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>-0.536708</td>\n",
       "      <td>0.704696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price_7</th>\n",
       "      <td>0.788682</td>\n",
       "      <td>0.758194</td>\n",
       "      <td>0.938184</td>\n",
       "      <td>0.795157</td>\n",
       "      <td>1.011346</td>\n",
       "      <td>0.765512</td>\n",
       "      <td>0.766708</td>\n",
       "      <td>0.691958</td>\n",
       "      <td>0.735239</td>\n",
       "      <td>0.777597</td>\n",
       "      <td>...</td>\n",
       "      <td>1.357613</td>\n",
       "      <td>1.362498</td>\n",
       "      <td>1.466718</td>\n",
       "      <td>1.401542</td>\n",
       "      <td>1.574646</td>\n",
       "      <td>1.473291</td>\n",
       "      <td>1.507547</td>\n",
       "      <td>1.500209</td>\n",
       "      <td>1.515571</td>\n",
       "      <td>1.557773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_volume_7</th>\n",
       "      <td>-0.224880</td>\n",
       "      <td>0.319666</td>\n",
       "      <td>-0.176008</td>\n",
       "      <td>-0.325096</td>\n",
       "      <td>-0.270762</td>\n",
       "      <td>-0.543869</td>\n",
       "      <td>-0.224880</td>\n",
       "      <td>-0.587163</td>\n",
       "      <td>-0.496894</td>\n",
       "      <td>-0.012719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303510</td>\n",
       "      <td>-0.587163</td>\n",
       "      <td>-0.587163</td>\n",
       "      <td>-0.587163</td>\n",
       "      <td>-0.226605</td>\n",
       "      <td>3.899436</td>\n",
       "      <td>-0.226605</td>\n",
       "      <td>0.683272</td>\n",
       "      <td>-0.587163</td>\n",
       "      <td>-0.226605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_8</th>\n",
       "      <td>0.787697</td>\n",
       "      <td>0.757734</td>\n",
       "      <td>0.936118</td>\n",
       "      <td>0.792727</td>\n",
       "      <td>1.008107</td>\n",
       "      <td>0.762843</td>\n",
       "      <td>0.763786</td>\n",
       "      <td>0.689694</td>\n",
       "      <td>0.732880</td>\n",
       "      <td>0.776596</td>\n",
       "      <td>...</td>\n",
       "      <td>1.361396</td>\n",
       "      <td>1.364500</td>\n",
       "      <td>1.463133</td>\n",
       "      <td>1.398865</td>\n",
       "      <td>1.573888</td>\n",
       "      <td>1.475550</td>\n",
       "      <td>1.507891</td>\n",
       "      <td>1.498460</td>\n",
       "      <td>1.518167</td>\n",
       "      <td>1.558189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_volume_8</th>\n",
       "      <td>2.734656</td>\n",
       "      <td>-0.222516</td>\n",
       "      <td>-0.470968</td>\n",
       "      <td>-0.236464</td>\n",
       "      <td>-0.326450</td>\n",
       "      <td>-0.222516</td>\n",
       "      <td>-0.506018</td>\n",
       "      <td>0.078848</td>\n",
       "      <td>-0.496479</td>\n",
       "      <td>-0.222516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.364334</td>\n",
       "      <td>-0.506018</td>\n",
       "      <td>0.187506</td>\n",
       "      <td>-0.499359</td>\n",
       "      <td>-0.336888</td>\n",
       "      <td>-0.505748</td>\n",
       "      <td>-0.223866</td>\n",
       "      <td>-0.223866</td>\n",
       "      <td>-0.302154</td>\n",
       "      <td>0.586505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price_8</th>\n",
       "      <td>0.788932</td>\n",
       "      <td>0.758485</td>\n",
       "      <td>0.937954</td>\n",
       "      <td>0.795701</td>\n",
       "      <td>1.011308</td>\n",
       "      <td>0.765900</td>\n",
       "      <td>0.766528</td>\n",
       "      <td>0.691939</td>\n",
       "      <td>0.734943</td>\n",
       "      <td>0.778084</td>\n",
       "      <td>...</td>\n",
       "      <td>1.357789</td>\n",
       "      <td>1.361889</td>\n",
       "      <td>1.467201</td>\n",
       "      <td>1.401342</td>\n",
       "      <td>1.574710</td>\n",
       "      <td>1.473773</td>\n",
       "      <td>1.507948</td>\n",
       "      <td>1.500709</td>\n",
       "      <td>1.514540</td>\n",
       "      <td>1.558152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_volume_8</th>\n",
       "      <td>-0.642968</td>\n",
       "      <td>-0.652908</td>\n",
       "      <td>-0.263913</td>\n",
       "      <td>0.581246</td>\n",
       "      <td>-0.652908</td>\n",
       "      <td>-0.629757</td>\n",
       "      <td>-0.035553</td>\n",
       "      <td>-0.263913</td>\n",
       "      <td>0.148604</td>\n",
       "      <td>0.168112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.583640</td>\n",
       "      <td>0.505929</td>\n",
       "      <td>0.315228</td>\n",
       "      <td>-0.265765</td>\n",
       "      <td>-0.652908</td>\n",
       "      <td>1.239593</td>\n",
       "      <td>-0.652908</td>\n",
       "      <td>2.179885</td>\n",
       "      <td>0.314857</td>\n",
       "      <td>-0.648648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_9</th>\n",
       "      <td>0.787490</td>\n",
       "      <td>0.757623</td>\n",
       "      <td>0.935687</td>\n",
       "      <td>0.792285</td>\n",
       "      <td>1.007702</td>\n",
       "      <td>0.762358</td>\n",
       "      <td>0.763459</td>\n",
       "      <td>0.689930</td>\n",
       "      <td>0.732471</td>\n",
       "      <td>0.776172</td>\n",
       "      <td>...</td>\n",
       "      <td>1.360961</td>\n",
       "      <td>1.365460</td>\n",
       "      <td>1.463433</td>\n",
       "      <td>1.398570</td>\n",
       "      <td>1.573470</td>\n",
       "      <td>1.475537</td>\n",
       "      <td>1.507565</td>\n",
       "      <td>1.499175</td>\n",
       "      <td>1.517803</td>\n",
       "      <td>1.558124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_volume_9</th>\n",
       "      <td>-0.280787</td>\n",
       "      <td>0.329400</td>\n",
       "      <td>-0.681564</td>\n",
       "      <td>-0.686783</td>\n",
       "      <td>-0.605404</td>\n",
       "      <td>0.150726</td>\n",
       "      <td>1.527351</td>\n",
       "      <td>0.518255</td>\n",
       "      <td>-0.538779</td>\n",
       "      <td>-0.686783</td>\n",
       "      <td>...</td>\n",
       "      <td>1.644105</td>\n",
       "      <td>-0.041093</td>\n",
       "      <td>-0.282720</td>\n",
       "      <td>-0.686719</td>\n",
       "      <td>-0.282720</td>\n",
       "      <td>-0.686268</td>\n",
       "      <td>-0.686783</td>\n",
       "      <td>0.526116</td>\n",
       "      <td>-0.586331</td>\n",
       "      <td>-0.686783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price_9</th>\n",
       "      <td>0.789275</td>\n",
       "      <td>0.758772</td>\n",
       "      <td>0.937281</td>\n",
       "      <td>0.796043</td>\n",
       "      <td>1.011745</td>\n",
       "      <td>0.765834</td>\n",
       "      <td>0.765912</td>\n",
       "      <td>0.692076</td>\n",
       "      <td>0.735448</td>\n",
       "      <td>0.778388</td>\n",
       "      <td>...</td>\n",
       "      <td>1.358033</td>\n",
       "      <td>1.362172</td>\n",
       "      <td>1.466061</td>\n",
       "      <td>1.401366</td>\n",
       "      <td>1.574422</td>\n",
       "      <td>1.474202</td>\n",
       "      <td>1.506765</td>\n",
       "      <td>1.501115</td>\n",
       "      <td>1.514906</td>\n",
       "      <td>1.558082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_volume_9</th>\n",
       "      <td>-0.536501</td>\n",
       "      <td>-0.169079</td>\n",
       "      <td>-0.212752</td>\n",
       "      <td>-0.212752</td>\n",
       "      <td>1.081471</td>\n",
       "      <td>-0.212752</td>\n",
       "      <td>-0.023158</td>\n",
       "      <td>-0.524632</td>\n",
       "      <td>0.490647</td>\n",
       "      <td>-0.212752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.254576</td>\n",
       "      <td>-0.214294</td>\n",
       "      <td>-0.536501</td>\n",
       "      <td>1.713349</td>\n",
       "      <td>-0.214294</td>\n",
       "      <td>0.490647</td>\n",
       "      <td>-0.536501</td>\n",
       "      <td>0.487564</td>\n",
       "      <td>-0.536501</td>\n",
       "      <td>-0.291930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_10</th>\n",
       "      <td>0.787187</td>\n",
       "      <td>0.757159</td>\n",
       "      <td>0.935795</td>\n",
       "      <td>0.793260</td>\n",
       "      <td>1.008016</td>\n",
       "      <td>0.762505</td>\n",
       "      <td>0.762996</td>\n",
       "      <td>0.689498</td>\n",
       "      <td>0.732142</td>\n",
       "      <td>0.776674</td>\n",
       "      <td>...</td>\n",
       "      <td>1.361299</td>\n",
       "      <td>1.365229</td>\n",
       "      <td>1.463096</td>\n",
       "      <td>1.398146</td>\n",
       "      <td>1.573441</td>\n",
       "      <td>1.475594</td>\n",
       "      <td>1.508197</td>\n",
       "      <td>1.498764</td>\n",
       "      <td>1.517394</td>\n",
       "      <td>1.557975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_volume_10</th>\n",
       "      <td>-0.063375</td>\n",
       "      <td>-0.178050</td>\n",
       "      <td>-0.476528</td>\n",
       "      <td>-0.232636</td>\n",
       "      <td>-0.521664</td>\n",
       "      <td>-0.308826</td>\n",
       "      <td>-0.232636</td>\n",
       "      <td>-0.232636</td>\n",
       "      <td>0.202351</td>\n",
       "      <td>0.251570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.521435</td>\n",
       "      <td>-0.521664</td>\n",
       "      <td>-0.521664</td>\n",
       "      <td>-0.234012</td>\n",
       "      <td>1.735150</td>\n",
       "      <td>-0.263461</td>\n",
       "      <td>-0.234012</td>\n",
       "      <td>-0.234012</td>\n",
       "      <td>-0.321899</td>\n",
       "      <td>0.248910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price_10</th>\n",
       "      <td>0.789235</td>\n",
       "      <td>0.758871</td>\n",
       "      <td>0.937074</td>\n",
       "      <td>0.796492</td>\n",
       "      <td>1.011317</td>\n",
       "      <td>0.766246</td>\n",
       "      <td>0.766324</td>\n",
       "      <td>0.692081</td>\n",
       "      <td>0.735725</td>\n",
       "      <td>0.778800</td>\n",
       "      <td>...</td>\n",
       "      <td>1.358446</td>\n",
       "      <td>1.362310</td>\n",
       "      <td>1.466447</td>\n",
       "      <td>1.401383</td>\n",
       "      <td>1.574467</td>\n",
       "      <td>1.474430</td>\n",
       "      <td>1.506991</td>\n",
       "      <td>1.500714</td>\n",
       "      <td>1.515230</td>\n",
       "      <td>1.557677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_volume_10</th>\n",
       "      <td>-0.521547</td>\n",
       "      <td>-0.203956</td>\n",
       "      <td>-0.465124</td>\n",
       "      <td>-0.521951</td>\n",
       "      <td>1.067267</td>\n",
       "      <td>0.234606</td>\n",
       "      <td>-0.203956</td>\n",
       "      <td>-0.513422</td>\n",
       "      <td>-0.161058</td>\n",
       "      <td>-0.521951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068063</td>\n",
       "      <td>-0.521951</td>\n",
       "      <td>0.371524</td>\n",
       "      <td>-0.521951</td>\n",
       "      <td>-0.518267</td>\n",
       "      <td>2.525471</td>\n",
       "      <td>-0.205470</td>\n",
       "      <td>-0.205470</td>\n",
       "      <td>-0.205470</td>\n",
       "      <td>-0.521951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_5</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_30</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_50</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 1089 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4         5     \\\n",
       "ask_price_1    0.787777  0.758016  0.939000  0.793726  1.009654  0.764239   \n",
       "ask_volume_1  -0.889437  0.309400  0.260860  0.596678 -0.794347 -0.399067   \n",
       "bid_price_1    0.787798  0.758037  0.939019  0.793746  1.009672  0.764260   \n",
       "bid_volume_1   0.369989 -0.802514 -0.789181 -0.867463  0.505927 -0.957800   \n",
       "ask_price_2    0.787888  0.757887  0.938915  0.793877  1.009834  0.763758   \n",
       "ask_volume_2  -0.125636 -0.244460 -0.244460 -0.434931 -0.246263 -0.028807   \n",
       "bid_price_2    0.788259  0.758442  0.938029  0.794147  1.009990  0.764017   \n",
       "bid_volume_2  -0.133476 -0.134059  0.074742  0.062083 -0.029738 -0.092298   \n",
       "ask_price_3    0.787568  0.758251  0.938615  0.793774  1.009680  0.763297   \n",
       "ask_volume_3  -0.262882  0.275781 -0.506480 -0.451355  0.016680 -0.247772   \n",
       "bid_price_3    0.788758  0.758572  0.938428  0.794547  1.010064  0.764441   \n",
       "bid_volume_3  -0.404985  0.829854 -0.116821 -0.125072  0.086748  0.363772   \n",
       "ask_price_4    0.787467  0.757949  0.938122  0.793320  1.009197  0.763075   \n",
       "ask_volume_4   0.058202 -0.246954 -0.396185 -0.235855  0.057952 -0.235855   \n",
       "bid_price_4    0.789295  0.758817  0.937796  0.794554  1.010428  0.764293   \n",
       "bid_volume_4  -0.305116 -0.199794 -0.199794 -0.221749  0.495750 -0.370369   \n",
       "ask_price_5    0.787550  0.757479  0.937631  0.792814  1.008930  0.763882   \n",
       "ask_volume_5  -0.224638  0.007922 -0.415813 -0.427709 -0.427709  0.242191   \n",
       "bid_price_5    0.789589  0.758447  0.937783  0.795064  1.010585  0.764727   \n",
       "bid_volume_5   0.966756 -0.446210  0.110235 -0.223583 -0.163872 -0.446175   \n",
       "ask_price_6    0.787083  0.757618  0.937101  0.792682  1.008603  0.763511   \n",
       "ask_volume_6  -0.245052 -0.245052 -0.505852  0.185717  0.681970  0.780493   \n",
       "bid_price_6    0.788530  0.757920  0.938107  0.795025  1.011041  0.765160   \n",
       "bid_volume_6  -0.161394 -0.203061 -0.334344 -0.091619 -0.203061  0.167475   \n",
       "ask_price_7    0.787987  0.757890  0.936605  0.792840  1.008546  0.762998   \n",
       "ask_volume_7  -0.079990  0.082992 -0.583127 -0.279858 -0.279858  0.461658   \n",
       "bid_price_7    0.788682  0.758194  0.938184  0.795157  1.011346  0.765512   \n",
       "bid_volume_7  -0.224880  0.319666 -0.176008 -0.325096 -0.270762 -0.543869   \n",
       "ask_price_8    0.787697  0.757734  0.936118  0.792727  1.008107  0.762843   \n",
       "ask_volume_8   2.734656 -0.222516 -0.470968 -0.236464 -0.326450 -0.222516   \n",
       "bid_price_8    0.788932  0.758485  0.937954  0.795701  1.011308  0.765900   \n",
       "bid_volume_8  -0.642968 -0.652908 -0.263913  0.581246 -0.652908 -0.629757   \n",
       "ask_price_9    0.787490  0.757623  0.935687  0.792285  1.007702  0.762358   \n",
       "ask_volume_9  -0.280787  0.329400 -0.681564 -0.686783 -0.605404  0.150726   \n",
       "bid_price_9    0.789275  0.758772  0.937281  0.796043  1.011745  0.765834   \n",
       "bid_volume_9  -0.536501 -0.169079 -0.212752 -0.212752  1.081471 -0.212752   \n",
       "ask_price_10   0.787187  0.757159  0.935795  0.793260  1.008016  0.762505   \n",
       "ask_volume_10 -0.063375 -0.178050 -0.476528 -0.232636 -0.521664 -0.308826   \n",
       "bid_price_10   0.789235  0.758871  0.937074  0.796492  1.011317  0.766246   \n",
       "bid_volume_10 -0.521547 -0.203956 -0.465124 -0.521951  1.067267  0.234606   \n",
       "label_5        3.000000  2.000000  3.000000  3.000000  3.000000  1.000000   \n",
       "label_10       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "label_20       1.000000  3.000000  3.000000  3.000000  3.000000  3.000000   \n",
       "label_30       3.000000  3.000000  3.000000  3.000000  3.000000  3.000000   \n",
       "label_50       3.000000  3.000000  3.000000  3.000000  3.000000  1.000000   \n",
       "\n",
       "                    6         7         8         9     ...      1079  \\\n",
       "ask_price_1     0.765947  0.691661  0.733535  0.778020  ...  1.358018   \n",
       "ask_volume_1    0.278482  0.051446 -1.074707 -0.853522  ... -0.871834   \n",
       "bid_price_1     0.765968  0.691683  0.733557  0.778041  ...  1.358031   \n",
       "bid_volume_1   -0.503068 -1.089764  0.068876  1.929023  ...  0.765229   \n",
       "ask_price_2     0.765466  0.691189  0.733266  0.777541  ...  1.359226   \n",
       "ask_volume_2   -0.447361 -0.447361 -0.447361 -0.400926  ... -0.447361   \n",
       "bid_price_2     0.765823  0.691919  0.734023  0.778052  ...  1.358191   \n",
       "bid_volume_2   -0.133178 -0.119246 -0.132025 -0.029546  ... -0.131514   \n",
       "ask_price_3     0.765163  0.690837  0.734156  0.777396  ...  1.359255   \n",
       "ask_volume_3    1.895348  0.078651 -0.262882  0.911670  ...  0.120005   \n",
       "bid_price_3     0.765736  0.692373  0.734314  0.778277  ...  1.357273   \n",
       "bid_volume_3   -0.241577 -0.397280 -0.216279 -0.216279  ... -0.418565   \n",
       "ask_price_4     0.764646  0.690310  0.733773  0.777058  ...  1.358820   \n",
       "ask_volume_4   21.450375  0.641376 -0.461455  0.182155  ...  0.494580   \n",
       "bid_price_4     0.766275  0.692740  0.734855  0.778815  ...  1.357768   \n",
       "bid_volume_4   -0.199794 -0.356205 -0.013859  0.520098  ...  0.506714   \n",
       "ask_price_5     0.764256  0.690247  0.733929  0.776708  ...  1.359432   \n",
       "ask_volume_5   -0.344202 -0.216562 -0.397014 -0.427709  ... -0.427709   \n",
       "bid_price_5     0.766473  0.692475  0.734291  0.778188  ...  1.357384   \n",
       "bid_volume_5    0.868956 -0.446210  0.012506 -0.223583  ...  0.933614   \n",
       "ask_price_6     0.763747  0.689671  0.733633  0.776299  ...  1.360536   \n",
       "ask_volume_6   -0.000361  0.059754  1.320388 -0.485056  ... -0.531791   \n",
       "bid_price_6     0.766181  0.692442  0.734766  0.778287  ...  1.357857   \n",
       "bid_volume_6   -0.215503 -0.463064 -0.203061 -0.463519  ... -0.267132   \n",
       "ask_price_7     0.763372  0.690212  0.733118  0.775945  ...  1.361692   \n",
       "ask_volume_7   -0.279858  4.405615 -0.279858 -0.279858  ... -0.621151   \n",
       "bid_price_7     0.766708  0.691958  0.735239  0.777597  ...  1.357613   \n",
       "bid_volume_7   -0.224880 -0.587163 -0.496894 -0.012719  ...  0.303510   \n",
       "ask_price_8     0.763786  0.689694  0.732880  0.776596  ...  1.361396   \n",
       "ask_volume_8   -0.506018  0.078848 -0.496479 -0.222516  ... -0.364334   \n",
       "bid_price_8     0.766528  0.691939  0.734943  0.778084  ...  1.357789   \n",
       "bid_volume_8   -0.035553 -0.263913  0.148604  0.168112  ... -0.583640   \n",
       "ask_price_9     0.763459  0.689930  0.732471  0.776172  ...  1.360961   \n",
       "ask_volume_9    1.527351  0.518255 -0.538779 -0.686783  ...  1.644105   \n",
       "bid_price_9     0.765912  0.692076  0.735448  0.778388  ...  1.358033   \n",
       "bid_volume_9   -0.023158 -0.524632  0.490647 -0.212752  ... -0.254576   \n",
       "ask_price_10    0.762996  0.689498  0.732142  0.776674  ...  1.361299   \n",
       "ask_volume_10  -0.232636 -0.232636  0.202351  0.251570  ... -0.521435   \n",
       "bid_price_10    0.766324  0.692081  0.735725  0.778800  ...  1.358446   \n",
       "bid_volume_10  -0.203956 -0.513422 -0.161058 -0.521951  ...  0.068063   \n",
       "label_5         1.000000  1.000000  1.000000  1.000000  ...  1.000000   \n",
       "label_10        1.000000  1.000000  1.000000  1.000000  ...  1.000000   \n",
       "label_20        3.000000  3.000000  1.000000  1.000000  ...  1.000000   \n",
       "label_30        3.000000  3.000000  3.000000  3.000000  ...  1.000000   \n",
       "label_50        1.000000  1.000000  1.000000  1.000000  ...  1.000000   \n",
       "\n",
       "                   1080      1081      1082      1083      1084      1085  \\\n",
       "ask_price_1    1.361493  1.464755  1.399127  1.574732  1.473707  1.508063   \n",
       "ask_volume_1   1.339390  1.550618  2.036066  0.100610 -1.124067  0.054699   \n",
       "bid_price_1    1.361505  1.464766  1.399139  1.574741  1.473718  1.508073   \n",
       "bid_volume_1  -0.769640 -0.715331 -0.693146 -0.452891 -0.307373  0.014105   \n",
       "ask_price_2    1.361327  1.464505  1.398750  1.574634  1.474931  1.507936   \n",
       "ask_volume_2   0.058041  0.196378 -0.245426 -0.434931 -0.447361  0.057461   \n",
       "bid_price_2    1.361724  1.465150  1.399530  1.574308  1.473806  1.508373   \n",
       "bid_volume_2  -0.134059 -0.128592 -0.118570 -0.092497 -0.092497  0.042280   \n",
       "ask_price_3    1.363712  1.464233  1.399314  1.574219  1.476094  1.507572   \n",
       "ask_volume_3  -0.264062 -0.495738 -0.234827 -0.264062 -0.510808 -0.420074   \n",
       "bid_price_3    1.362101  1.465473  1.399862  1.573928  1.474206  1.508768   \n",
       "bid_volume_3  -0.360682  0.437514 -0.376509 -0.418565 -0.043240  9.708950   \n",
       "ask_price_4    1.363376  1.463812  1.398884  1.574952  1.477187  1.507176   \n",
       "ask_volume_4  -0.318562 -0.236929  0.662429 -0.318562  1.005394 -0.361169   \n",
       "bid_price_4    1.362341  1.465901  1.400295  1.573779  1.473633  1.507741   \n",
       "bid_volume_4  -0.286667 -0.366334 -0.281325 -0.222695 -0.097393  0.323005   \n",
       "ask_price_5    1.365580  1.463355  1.399305  1.574625  1.476771  1.507077   \n",
       "ask_volume_5   0.233478  2.406304 -0.217567 -0.427709  0.550852  1.230201   \n",
       "bid_price_5    1.361878  1.465978  1.400751  1.574003  1.473592  1.508109   \n",
       "bid_volume_5  -0.048088  0.075503  0.966756 -0.330250  0.495319 -0.446210   \n",
       "ask_price_6    1.365074  1.463056  1.399666  1.574611  1.476296  1.506822   \n",
       "ask_volume_6   7.431290 -0.527422 -0.531791 -0.527923 -0.531791 -0.246417   \n",
       "bid_price_6    1.362350  1.466268  1.401104  1.574364  1.473724  1.508258   \n",
       "bid_volume_6   0.007877 -0.204301 -0.463519  1.634821 -0.204301 -0.204301   \n",
       "ask_price_7    1.364914  1.463239  1.399215  1.574275  1.475930  1.506813   \n",
       "ask_volume_7  -0.281483 -0.025825  0.012090 -0.617197 -0.034058 -0.621151   \n",
       "bid_price_7    1.362498  1.466718  1.401542  1.574646  1.473291  1.507547   \n",
       "bid_volume_7  -0.587163 -0.587163 -0.587163 -0.226605  3.899436 -0.226605   \n",
       "ask_price_8    1.364500  1.463133  1.398865  1.573888  1.475550  1.507891   \n",
       "ask_volume_8  -0.506018  0.187506 -0.499359 -0.336888 -0.505748 -0.223866   \n",
       "bid_price_8    1.361889  1.467201  1.401342  1.574710  1.473773  1.507948   \n",
       "bid_volume_8   0.505929  0.315228 -0.265765 -0.652908  1.239593 -0.652908   \n",
       "ask_price_9    1.365460  1.463433  1.398570  1.573470  1.475537  1.507565   \n",
       "ask_volume_9  -0.041093 -0.282720 -0.686719 -0.282720 -0.686268 -0.686783   \n",
       "bid_price_9    1.362172  1.466061  1.401366  1.574422  1.474202  1.506765   \n",
       "bid_volume_9  -0.214294 -0.536501  1.713349 -0.214294  0.490647 -0.536501   \n",
       "ask_price_10   1.365229  1.463096  1.398146  1.573441  1.475594  1.508197   \n",
       "ask_volume_10 -0.521664 -0.521664 -0.234012  1.735150 -0.263461 -0.234012   \n",
       "bid_price_10   1.362310  1.466447  1.401383  1.574467  1.474430  1.506991   \n",
       "bid_volume_10 -0.521951  0.371524 -0.521951 -0.518267  2.525471 -0.205470   \n",
       "label_5        1.000000  3.000000  1.000000  3.000000  1.000000  3.000000   \n",
       "label_10       1.000000  1.000000  1.000000  1.000000  1.000000  3.000000   \n",
       "label_20       1.000000  3.000000  1.000000  3.000000  3.000000  3.000000   \n",
       "label_30       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "label_50       1.000000  3.000000  1.000000  1.000000  3.000000  3.000000   \n",
       "\n",
       "                   1086      1087      1088  \n",
       "ask_price_1    1.498541  1.518134  1.556828  \n",
       "ask_volume_1   1.058963  1.625680 -0.937288  \n",
       "bid_price_1    1.498552  1.518144  1.556838  \n",
       "bid_volume_1  -0.765127 -0.956201  1.235708  \n",
       "ask_price_2    1.498159  1.518127  1.557788  \n",
       "ask_volume_2  -0.087993 -0.245426 -0.245426  \n",
       "bid_price_2    1.498932  1.518522  1.557034  \n",
       "bid_volume_2  -0.092497 -0.092371 -0.119883  \n",
       "ask_price_3    1.498814  1.519629  1.557587  \n",
       "ask_volume_3  -0.264062 -0.264062 -0.117337  \n",
       "bid_price_3    1.499171  1.518758  1.557402  \n",
       "bid_volume_3  -0.411983 -0.389768  2.325632  \n",
       "ask_price_4    1.498496  1.519490  1.557179  \n",
       "ask_volume_4  -0.042872  0.483302 -0.365680  \n",
       "bid_price_4    1.499656  1.519045  1.557549  \n",
       "bid_volume_4  -0.373096 -0.375127 -0.200629  \n",
       "ask_price_5    1.498455  1.519039  1.557615  \n",
       "ask_volume_5   0.623334 -0.129737 -0.217567  \n",
       "bid_price_5    1.499965  1.517881  1.556971  \n",
       "bid_volume_5  -0.446210 -0.446210 -0.446210  \n",
       "ask_price_6    1.499121  1.518686  1.557816  \n",
       "ask_volume_6  -0.531791 -0.531791 -0.531791  \n",
       "bid_price_6    1.500135  1.516735  1.557431  \n",
       "bid_volume_6   0.573643 -0.463519  0.184258  \n",
       "ask_price_7    1.498679  1.518246  1.558028  \n",
       "ask_volume_7   0.012090 -0.536708  0.704696  \n",
       "bid_price_7    1.500209  1.515571  1.557773  \n",
       "bid_volume_7   0.683272 -0.587163 -0.226605  \n",
       "ask_price_8    1.498460  1.518167  1.558189  \n",
       "ask_volume_8  -0.223866 -0.302154  0.586505  \n",
       "bid_price_8    1.500709  1.514540  1.558152  \n",
       "bid_volume_8   2.179885  0.314857 -0.648648  \n",
       "ask_price_9    1.499175  1.517803  1.558124  \n",
       "ask_volume_9   0.526116 -0.586331 -0.686783  \n",
       "bid_price_9    1.501115  1.514906  1.558082  \n",
       "bid_volume_9   0.487564 -0.536501 -0.291930  \n",
       "ask_price_10   1.498764  1.517394  1.557975  \n",
       "ask_volume_10 -0.234012 -0.321899  0.248910  \n",
       "bid_price_10   1.500714  1.515230  1.557677  \n",
       "bid_volume_10 -0.205470 -0.205470 -0.521951  \n",
       "label_5        1.000000  1.000000  1.000000  \n",
       "label_10       1.000000  3.000000  1.000000  \n",
       "label_20       3.000000  3.000000  1.000000  \n",
       "label_30       1.000000  1.000000  1.000000  \n",
       "label_50       1.000000  1.000000  1.000000  \n",
       "\n",
       "[45 rows x 1089 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40171c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c59e24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_x(data):\n",
    "    df1 = data[:67, :].T\n",
    "    return np.array(df1)\n",
    "\n",
    "def get_label(data):\n",
    "    lob = data[-5:, :].T\n",
    "    return lob\n",
    "\n",
    "def data_classification(X, Y, T):\n",
    "    [N, D] = X.shape\n",
    "    df = np.array(X)\n",
    "\n",
    "    dY = np.array(Y)\n",
    "\n",
    "    dataY = dY[T - 1:N]\n",
    "\n",
    "    dataX = np.zeros((N - T + 1, T, D))\n",
    "    for i in range(T, N + 1):\n",
    "        dataX[i - T] = df[i - T:i, :]\n",
    "\n",
    "    return dataX, dataY\n",
    "\n",
    "def torch_data(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    x = torch.unsqueeze(x, 1)\n",
    "    y = torch.from_numpy(y)\n",
    "    y = F.one_hot(y, num_classes=3)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9255e3d3-ab64-4420-9119-5561db42fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, data, k, num_classes, T):\n",
    "        \"\"\"Initialization\"\"\" \n",
    "        self.k = k\n",
    "        self.num_classes = num_classes\n",
    "        self.T = T\n",
    "            \n",
    "        x = prepare_x(data)\n",
    "        y = get_label(data)\n",
    "        x, y = data_classification(x, y, self.T)\n",
    "        y = y[:,self.k] - 1\n",
    "        self.length = len(x)\n",
    "\n",
    "        x = torch.from_numpy(x)\n",
    "        self.x = torch.unsqueeze(x, 1)\n",
    "        self.y = torch.from_numpy(y.astype(np.int64))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates samples of data\"\"\"\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc293b4-b780-4abc-9363-9f33e5ca8ba0",
   "metadata": {},
   "source": [
    "Data preparation\n",
    "We used no auction dataset that is normalised by decimal precision approach in their work. The first seven days are training data and the last three days are testing data. A validation set (20%) from the training set is used to monitor the overfitting behaviours.\n",
    "\n",
    "The first 40 columns of the FI-2010 dataset are 10 levels ask and bid information for a limit order book and we only use these 40 features in our network. The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856d805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 871) (45, 109) (45, 109)\n"
     ]
    }
   ],
   "source": [
    "# please change the data_path to your local path\n",
    "# data_path = '/nfs/home/zihaoz/limit_order_book/data'\n",
    "\n",
    "# dec_data = np.loadtxt('../data/data/Train_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "# dec_train = dec_data[:, :int(np.floor(dec_data.shape[1] * 0.8))]\n",
    "# dec_val = dec_data[:, int(np.floor(dec_data.shape[1] * 0.8)):]\n",
    "\n",
    "# dec_test1 = np.loadtxt('../data/data/Test_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "# dec_test2 = np.loadtxt('../data/data/Test_Dst_NoAuction_DecPre_CF_8.txt')\n",
    "# dec_test3 = np.loadtxt('../data/data/Test_Dst_NoAuction_DecPre_CF_9.txt')\n",
    "# dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "\n",
    "# Split into training (80%), validation (10%), and test (10%) without shuffling (time series)\n",
    "num_samples = len(final_df.T)\n",
    "train_end = int(num_samples * 0.8)\n",
    "val_end = int(num_samples * 0.9)\n",
    "final_df_np = final_df.values\n",
    "\n",
    "dec_train = final_df_np[:, :train_end]\n",
    "dec_val = final_df_np[:, train_end:val_end]\n",
    "dec_test = final_df_np[:, val_end:]\n",
    "\n",
    "print(dec_train.shape, dec_val.shape, dec_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966a9cc7-aa18-4b00-8890-90f7a3801c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([822, 1, 50, 45]) torch.Size([822])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataset_train = Dataset(data=dec_train, k=4, num_classes=3, T=50)\n",
    "dataset_val = Dataset(data=dec_val, k=4, num_classes=3, T=50)\n",
    "dataset_test = Dataset(data=dec_test, k=4, num_classes=3, T=50)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(dataset_train.x.shape, dataset_train.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8fadd16-9679-4e58-9f12-04815faf24e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.1014,  0.7856, -2.1014,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-2.2486, -0.7438, -2.2486,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-2.3499,  0.3212, -2.3498,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.3261, -1.0457, -0.3260,  ...,  3.0000,  3.0000,  3.0000],\n",
      "          [-0.7385, -1.1196, -0.7385,  ...,  3.0000,  1.0000,  1.0000],\n",
      "          [-0.6338,  0.4428, -0.6338,  ...,  3.0000,  1.0000,  1.0000]]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([0])\n",
      "torch.Size([1, 1, 50, 45]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "tmp_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=1, shuffle=True)\n",
    "\n",
    "for x, y in tmp_loader:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5831b89-74f3-4b96-811f-e0dd3f4f9cb7",
   "metadata": {},
   "source": [
    "Model Architecture\n",
    "Please find the detailed discussion of our model architecture in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fce4666-5f23-4324-87cf-69279998f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class deeplob(nn.Module):\n",
    "    def __init__(self, y_len):\n",
    "        super().__init__()\n",
    "        self.y_len = y_len\n",
    "        \n",
    "        # convolution blocks\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "#             nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,10)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        \n",
    "        # inception moduels\n",
    "        self.inp1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.inp2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.inp3 = nn.Sequential(\n",
    "            nn.MaxPool2d((3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        \n",
    "        # lstm layers\n",
    "        self.lstm = nn.LSTM(input_size=384, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        # self.lstm = nn.LSTM(input_size=192, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, self.y_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "    \n",
    "        # Initial hidden and cell state for LSTM\n",
    "        h0 = torch.zeros(1, batch_size, 64).to(x.device)\n",
    "        c0 = torch.zeros(1, batch_size, 64).to(x.device)\n",
    "    \n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)       # (B, C1, T, D)\n",
    "        x = self.conv2(x)       # (B, C2, T, D)\n",
    "        x = self.conv3(x)       # (B, C3, T, D)\n",
    "    \n",
    "        # Inception-like branches\n",
    "        x_inp1 = self.inp1(x)   # e.g., 1x1 conv\n",
    "        x_inp2 = self.inp2(x)   # e.g., 3x1 conv\n",
    "        x_inp3 = self.inp3(x)   # e.g., 5x1 conv\n",
    "    \n",
    "        # Concatenate across the channel dimension\n",
    "        x = torch.cat((x_inp1, x_inp2, x_inp3), dim=1)  # shape: (B, C_total, T, D)\n",
    "    \n",
    "        # Prepare input for LSTM\n",
    "        x = x.permute(0, 2, 1, 3)                 # (B, T, C, D)\n",
    "        x = x.reshape(batch_size, x.shape[1], -1) # (B, T, C*D)\n",
    "    \n",
    "        # LSTM expects input of shape (B, T, input_size)\n",
    "        x, _ = self.lstm(x, (h0, c0))             # output: (B, T, hidden_size)\n",
    "    \n",
    "        # Use last time step's hidden state\n",
    "        x = x[:, -1, :]                           # (B, hidden_size)\n",
    "    \n",
    "        # Fully connected + softmax for classification\n",
    "        x = self.fc1(x)                           # (B, num_classes)\n",
    "        forecast_y = torch.softmax(x, dim=1)\n",
    "    \n",
    "        return forecast_y\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # h0: (number of hidden layers, batch size, hidden size)\n",
    "#         h0 = torch.zeros(1, x.size(0), 64).to(device)\n",
    "#         c0 = torch.zeros(1, x.size(0), 64).to(device)\n",
    "    \n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "        \n",
    "#         x_inp1 = self.inp1(x)\n",
    "#         x_inp2 = self.inp2(x)\n",
    "#         x_inp3 = self.inp3(x)  \n",
    "        \n",
    "#         x = torch.cat((x_inp1, x_inp2, x_inp3), dim=1)\n",
    "        \n",
    "# #         x = torch.transpose(x, 1, 2)\n",
    "#         x = x.permute(0, 2, 1, 3)\n",
    "#         x = torch.reshape(x, (-1, x.shape[1], x.shape[2]))\n",
    "        \n",
    "#         x, _ = self.lstm(x, (h0, c0))\n",
    "#         x = x[:, -1, :]\n",
    "#         x = self.fc1(x)\n",
    "#         forecast_y = torch.softmax(x, dim=1)\n",
    "        \n",
    "#         return forecast_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77825bcc-35c0-483c-bdf6-a57533c95c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deeplob(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(1, 2), stride=(1, 2))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 2))\n",
       "    (1): Tanh()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (4): Tanh()\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (7): Tanh()\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(1, 10), stride=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (inp1): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=same)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (inp2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(5, 1), stride=(1, 1), padding=same)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (inp3): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
       "    (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (lstm): LSTM(384, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = deeplob(y_len = dataset_train.num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecad4157-0a0e-49b2-8a57-2973b81c224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, (1, 1, 100, 66))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb249303-aaba-41be-b878-6b800b1b0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8b2cd-928d-4e6a-aab8-ef375d95bf66",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4723a4bf-581c-45f1-ad3c-e92b20d8cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to encapsulate the training loop\n",
    "def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):\n",
    "    \n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "    best_test_loss = np.inf\n",
    "    best_test_epoch = 0\n",
    "\n",
    "    for it in tqdm(range(epochs)):\n",
    "        \n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        for inputs, targets in train_loader:\n",
    "            # move data to GPU\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "            # print(\"inputs.shape:\", inputs.shape)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            # print(\"about to get model output\")\n",
    "            outputs = model(inputs)\n",
    "            # print(\"done getting model output\")\n",
    "            # print(\"outputs.shape:\", outputs.shape, \"targets.shape:\", targets.shape)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # print(\"about to optimize\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        # Get train loss and test loss\n",
    "        train_loss = np.mean(train_loss) # a little misleading\n",
    "    \n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)      \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss.append(loss.item())\n",
    "        test_loss = np.mean(test_loss)\n",
    "\n",
    "        # Save losses\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "        \n",
    "        if test_loss < best_test_loss:\n",
    "            torch.save(model, './best_val_model_pytorch')\n",
    "            best_test_loss = test_loss\n",
    "            best_test_epoch = it\n",
    "            print('model saved')\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "          Validation Loss: {test_loss:.4f}, Duration: {dt}, Best Val Epoch: {best_test_epoch}')\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5f8b13b-f836-483c-8611-a4ffb66debf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▌                           | 1/50 [00:21<17:23, 21.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 1/50, Train Loss: 1.0791,           Validation Loss: 1.0755, Duration: 0:00:21.286728, Best Val Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█                           | 2/50 [00:26<09:15, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 2/50, Train Loss: 1.0312,           Validation Loss: 1.0611, Duration: 0:00:04.777107, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|█▋                          | 3/50 [00:30<06:38,  8.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 3/50, Train Loss: 0.9880,           Validation Loss: 1.0363, Duration: 0:00:04.779426, Best Val Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██▏                         | 4/50 [00:35<05:23,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 4/50, Train Loss: 0.9378,           Validation Loss: 1.0084, Duration: 0:00:04.849954, Best Val Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|██▊                         | 5/50 [00:40<04:40,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 5/50, Train Loss: 0.8839,           Validation Loss: 0.9595, Duration: 0:00:04.813584, Best Val Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███▎                        | 6/50 [00:45<04:12,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 6/50, Train Loss: 0.8237,           Validation Loss: 0.9023, Duration: 0:00:04.763123, Best Val Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███▉                        | 7/50 [00:50<03:52,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 7/50, Train Loss: 0.7653,           Validation Loss: 0.8336, Duration: 0:00:04.748513, Best Val Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|████▍                       | 8/50 [00:54<03:38,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 8/50, Train Loss: 0.7171,           Validation Loss: 0.7841, Duration: 0:00:04.768308, Best Val Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█████                       | 9/50 [00:59<03:27,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 9/50, Train Loss: 0.6800,           Validation Loss: 0.7360, Duration: 0:00:04.753124, Best Val Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█████▍                     | 10/50 [01:04<03:18,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 10/50, Train Loss: 0.6578,           Validation Loss: 0.7107, Duration: 0:00:04.733044, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████▉                     | 11/50 [01:09<03:11,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 11/50, Train Loss: 0.6397,           Validation Loss: 0.7044, Duration: 0:00:04.756163, Best Val Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████▍                    | 12/50 [01:13<03:04,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 12/50, Train Loss: 0.6288,           Validation Loss: 0.6953, Duration: 0:00:04.783485, Best Val Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|███████                    | 13/50 [01:18<02:59,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 13/50, Train Loss: 0.6213,           Validation Loss: 0.6870, Duration: 0:00:04.786971, Best Val Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|███████▌                   | 14/50 [01:23<02:53,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 14/50, Train Loss: 0.6151,           Validation Loss: 0.6789, Duration: 0:00:04.758481, Best Val Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████                   | 15/50 [01:28<02:48,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 15/50, Train Loss: 0.6101,           Validation Loss: 0.6753, Duration: 0:00:04.777814, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|████████▋                  | 16/50 [01:32<02:42,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 16/50, Train Loss: 0.6068,           Validation Loss: 0.6672, Duration: 0:00:04.760038, Best Val Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|█████████▏                 | 17/50 [01:37<02:37,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Loss: 0.6033,           Validation Loss: 0.6756, Duration: 0:00:04.755580, Best Val Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████▋                 | 18/50 [01:42<02:35,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Loss: 0.5995,           Validation Loss: 0.6952, Duration: 0:00:04.994412, Best Val Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|██████████▎                | 19/50 [01:47<02:29,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Loss: 0.5967,           Validation Loss: 0.6718, Duration: 0:00:04.797317, Best Val Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████▊                | 20/50 [01:52<02:24,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 20/50, Train Loss: 0.5933,           Validation Loss: 0.6629, Duration: 0:00:04.781741, Best Val Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|███████████▎               | 21/50 [01:56<02:19,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Loss: 0.5935,           Validation Loss: 0.6641, Duration: 0:00:04.758417, Best Val Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|███████████▉               | 22/50 [02:02<02:16,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 22/50, Train Loss: 0.5897,           Validation Loss: 0.6599, Duration: 0:00:05.073435, Best Val Epoch: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████████████▍              | 23/50 [02:06<02:11,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 23/50, Train Loss: 0.5916,           Validation Loss: 0.6591, Duration: 0:00:04.831543, Best Val Epoch: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████▉              | 24/50 [02:11<02:05,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Loss: 0.5884,           Validation Loss: 0.6663, Duration: 0:00:04.786857, Best Val Epoch: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████▌             | 25/50 [02:16<02:01,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 25/50, Train Loss: 0.5848,           Validation Loss: 0.6554, Duration: 0:00:04.892069, Best Val Epoch: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|██████████████             | 26/50 [02:21<01:56,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Loss: 0.5854,           Validation Loss: 0.6604, Duration: 0:00:04.786115, Best Val Epoch: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|██████████████▌            | 27/50 [02:26<01:50,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 27/50, Train Loss: 0.5825,           Validation Loss: 0.6548, Duration: 0:00:04.776279, Best Val Epoch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|███████████████            | 28/50 [02:30<01:46,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Loss: 0.5817,           Validation Loss: 0.6572, Duration: 0:00:04.817037, Best Val Epoch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████▋           | 29/50 [02:35<01:40,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Loss: 0.5795,           Validation Loss: 0.6557, Duration: 0:00:04.779216, Best Val Epoch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████▏          | 30/50 [02:40<01:36,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Loss: 0.5787,           Validation Loss: 0.6589, Duration: 0:00:04.788365, Best Val Epoch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████▋          | 31/50 [02:45<01:31,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Train Loss: 0.5772,           Validation Loss: 0.6579, Duration: 0:00:04.780590, Best Val Epoch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|█████████████████▎         | 32/50 [02:52<01:39,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Train Loss: 0.5769,           Validation Loss: 0.6564, Duration: 0:00:07.213916, Best Val Epoch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|█████████████████▊         | 33/50 [02:57<01:30,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 33/50, Train Loss: 0.5762,           Validation Loss: 0.6514, Duration: 0:00:04.830840, Best Val Epoch: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████████████████▎        | 34/50 [03:02<01:22,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Train Loss: 0.5754,           Validation Loss: 0.6617, Duration: 0:00:04.812031, Best Val Epoch: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████▉        | 35/50 [03:06<01:15,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Train Loss: 0.5755,           Validation Loss: 0.6576, Duration: 0:00:04.778531, Best Val Epoch: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████████████████▍       | 36/50 [03:11<01:09,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 36/50, Train Loss: 0.5750,           Validation Loss: 0.6484, Duration: 0:00:04.831960, Best Val Epoch: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████████████████▉       | 37/50 [03:16<01:04,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 37/50, Train Loss: 0.5745,           Validation Loss: 0.6460, Duration: 0:00:04.815452, Best Val Epoch: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|████████████████████▌      | 38/50 [03:21<00:58,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 38/50, Train Loss: 0.5742,           Validation Loss: 0.6445, Duration: 0:00:04.775559, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████      | 39/50 [03:26<00:53,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Train Loss: 0.5733,           Validation Loss: 0.6464, Duration: 0:00:04.770332, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████▌     | 40/50 [03:30<00:48,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Train Loss: 0.5734,           Validation Loss: 0.6507, Duration: 0:00:04.765375, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████▏    | 41/50 [03:35<00:43,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Train Loss: 0.5724,           Validation Loss: 0.6600, Duration: 0:00:04.765444, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|██████████████████████▋    | 42/50 [03:40<00:38,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Train Loss: 0.5722,           Validation Loss: 0.6708, Duration: 0:00:04.770450, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████▏   | 43/50 [03:45<00:33,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Train Loss: 0.5718,           Validation Loss: 0.6624, Duration: 0:00:04.756090, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|███████████████████████▊   | 44/50 [03:49<00:28,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Train Loss: 0.5713,           Validation Loss: 0.6689, Duration: 0:00:04.752590, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████▎  | 45/50 [03:54<00:23,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Train Loss: 0.5708,           Validation Loss: 0.6755, Duration: 0:00:04.755038, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|████████████████████████▊  | 46/50 [03:59<00:19,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Train Loss: 0.5705,           Validation Loss: 0.6640, Duration: 0:00:04.760185, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████▍ | 47/50 [04:04<00:14,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Train Loss: 0.5700,           Validation Loss: 0.6662, Duration: 0:00:04.755766, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████████████████████▉ | 48/50 [04:08<00:09,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Train Loss: 0.5703,           Validation Loss: 0.6666, Duration: 0:00:04.753533, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|██████████████████████████▍| 49/50 [04:13<00:04,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Loss: 0.5693,           Validation Loss: 0.6697, Duration: 0:00:04.752305, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 50/50 [04:18<00:00,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.5695,           Validation Loss: 0.6776, Duration: 0:00:04.747603, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = batch_gd(model, criterion, optimizer, \n",
    "                                    train_loader, val_loader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdf77edd-c710-4f4f-9589-4eb67a945f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f94f01d6bb0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc4be3-cc95-4b3c-a1cf-3586e7c3b3cf",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2c43249-e4e6-4ebd-a953-2fb80c10af6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('best_val_model_pytorch')\n",
    "\n",
    "n_correct = 0.\n",
    "n_total = 0.\n",
    "for inputs, targets in test_loader:\n",
    "    # Move to GPU\n",
    "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    # update counts\n",
    "    n_correct += (predictions == targets).sum().item()\n",
    "    n_total += targets.shape[0]\n",
    "\n",
    "test_acc = n_correct / n_total\n",
    "print(f\"Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2bf84c6-baba-4739-905a-e80c71d58585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('best_val_model_pytorch')\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    # Move to GPU\n",
    "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    all_targets.append(targets.cpu().numpy())\n",
    "    all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "all_targets = np.concatenate(all_targets)    \n",
    "all_predictions = np.concatenate(all_predictions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7d0da7b-ee40-48e0-9c97-066b4dc3505f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5294    0.6923    0.6000        26\n",
      "           1     0.0000    0.0000    0.0000         3\n",
      "           2     0.6923    0.5806    0.6316        31\n",
      "\n",
      "    accuracy                         0.6000        60\n",
      "   macro avg     0.4072    0.4243    0.4105        60\n",
      "weighted avg     0.5871    0.6000    0.5863        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lydiali/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/lydiali/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/lydiali/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44decf8b-650d-494f-b370-17c5ced7ac3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
